---
title: "Homework4"
author: "Xingyuan Chen"
date: "2019/4/10"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain $p + 1$ models, containing $0, 1, 2, . . . , p$ predictors. Explain your answers:  

(a) Which of the three models with k predictors has the smallest training RSS?  

**Answer:**  
Best subset selection.

(b) Which of the three models with k predictors has the smallest test RSS?  

**Answer:**  
All models may have the smallest test RSS, depend which is closest to the real true model.


(c) True or False:
i. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.  
True.  
ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.  
True.
iii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.  
False.  
iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.  
False.  
v. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k+1)-variable model identified by best subset selection.  
False.

## 2. For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer.  

(a) The lasso, relative to least squares, is:  
i. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.  
ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.  
iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.  
iv. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.  
**Answer:**iii.  

(b) Repeat (a) for ridge regression relative to least squares.  
**Answer:**iii.

## 8. In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.  

(a) Use the rnorm() function to generate a predictor $X$ of length $n = 100$, as well as a noise vector $\epsilon$ of length $n = 100$.
```{r}
set.seed(1)
X = rnorm(100)
eps = rnorm(100)
```

(b) Generate a response vector Y of length n = 100 according to the model
$$Y=\beta_0+\beta_1X+\beta_2X^2+\beta_3X^3+\epsilon$$
where $\beta_0,\beta_1,\beta_2,\beta_3$ are constants of your choice.  
```{r}
b0 = 1
b1 = 2
b2 = 3
b3 = 4
Y = b0 + b1 * X + b2 * X^2 + b3 * X^3 + eps
```

(c) Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors $X,X^2, . . .,X^{10}$. What is the best model obtained according to $C_p$, BIC, and adjusted $R^2$? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both $X$ and $Y$.
```{r}
library(leaps)
data = data.frame(y = Y, x = X)
model = regsubsets(y ~ poly(x, 10, raw = T), data = data, nvmax = 10)
summary = summary(model)
which.min(summary$cp)
which.min(summary$bic)
which.max(summary$adjr2)
par(mfrow = c(2, 2))
plot(summary$cp)
plot(summary$bic)
plot(summary$adjr2)
coefficients(model, id=3)
coefficients(model, id=4)
```

(d) Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?
```{r}
fwd = regsubsets(y ~ poly(x, 10, raw = T), data = data, nvmax = 10, 
    method = "forward")
bwd = regsubsets(y ~ poly(x, 10, raw = T), data = data, nvmax = 10, 
    method = "backward")
fwd.summary = summary(fwd)
bwd.summary = summary(bwd)
which.min(fwd.summary$cp)
which.min(bwd.summary$cp)
which.min(fwd.summary$bic)
which.min(bwd.summary$bic)
which.max(fwd.summary$adjr2)
which.max(bwd.summary$adjr2)
par(mfrow = c(2, 2))
plot(fwd.summary$cp)
plot(fwd.summary$bic)
plot(fwd.summary$adjr2)
par(mfrow = c(2, 2))
plot(bwd.summary$cp)
plot(bwd.summary$bic)
plot(bwd.summary$adjr2)
```

Answer is the same. 

(e) Now fit a lasso model to the simulated data, again using $X,X^2, . . . , X^{10}$ as predictors. Use cross-validation to select the optimal value of $\lambda$. Create plots of the cross-validation error as a function of $\lambda$. Report the resulting coefficient estimates, and discuss the results obtained.  
```{r}
library(glmnet)
xmatrix = model.matrix(y ~ poly(x, 10, raw = T), data = data)[, -1]
lasso = cv.glmnet(xmatrix, Y, alpha = 1)
minlambda = lasso$lambda.min
minlambda
plot(lasso)
bestlasso = glmnet(xmatrix, Y, alpha = 1)
predict(bestlasso,s=minlambda, type = "coefficients")
```

Besides 1,2,3,5, lasso also picked 4 and 7.  

(f) Now generate a response vector $Y$ according to the model $Y = \beta_0 + \beta_7X^7 + \epsilon$, and perform best subset selection and the lasso. Discuss the results obtained.  
```{r 7}
Y = b0 + 7 * X^7 + eps
dataf = data.frame(y = Y, x = X)
modelf = regsubsets(y ~ poly(x, 10, raw = T), data = data, nvmax = 10)
modelfsummary = summary(modelf)
which.min(modelfsummary$cp)
which.min(modelfsummary$bic)
which.max(modelfsummary$adjr2)
coefficients(modelf, id = 3)
coefficients(modelf, id = 4)

xfmatrix = model.matrix(y ~ poly(x, 10, raw = T), data = dataf)[, -1]
lassof = cv.glmnet(xfmatrix, Y, alpha=1)
minlambdaf = lassof$lambda.min
minlambdaf
bestlassof = glmnet(xfmatrix, Y, alpha = 1)
predict(bestlassof, s = minlambdaf, type = "coefficients")
```

Best subset picked 1,2,3,(5), while lasso only picked 7.